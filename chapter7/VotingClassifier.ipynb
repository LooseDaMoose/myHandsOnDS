{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8\n",
    "Load the MNIST data (introduced in Chapter 3), and split it into a training set, a\n",
    "validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation,\n",
    "and 10,000 for testing). Then train various classifiers, such as a Random\n",
    "Forest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine\n",
    "them into an ensemble that outperforms them all on the validation set, using a\n",
    "soft or hard voting classifier. Once you have found one, try it on the test set. How\n",
    "much better does it perform compared to the individual classifiers?\n",
    "\n",
    "https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(\"mnist_784\", version = 1) # divided into [\"data\"] and [\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset mnist dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, vl, ts = 50000, 60000, 70000\n",
    "train, val, test = [x for x in range(tr)], [x for x in range(tr, vl)], [x for x in range(vl, ts)]\n",
    "X_train, X_val, X_test = mnist[\"data\"][train], mnist[\"data\"][val], mnist[\"data\"][test]\n",
    "y_train, y_val, y_test = mnist[\"target\"][train], mnist[\"target\"][val], mnist[\"target\"][test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "#winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classifiers: Random Forest, Boosted Trees, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import os.path\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We will perform logistic regression using stochastic gradient descent to fit the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "if os.path.isfile(\"logregMNIST.pkl\") == False:\n",
    "    logreg_clf = Pipeline([(\"scale\", StandardScaler()),                   \n",
    "                        (\"logregd_clf\", SGDClassifier(loss = \"log\"))])\n",
    "\n",
    "\n",
    "    time0 = time.time()\n",
    "    logreg_clf.fit(X_train, y_train)\n",
    "    time1 = time.time()\n",
    "    print(\"Training time: {}\".format(time1 - time0))\n",
    "    winsound.Beep(freq, duration)\n",
    "else:\n",
    "    logreg_clf = joblib.load(\"logregMNIST.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred time: 0.40926337242126465\n"
     ]
    }
   ],
   "source": [
    "# training metrics\n",
    "time0 = time.time()\n",
    "y_tr_pred = logreg_clf.predict(X_train)\n",
    "time1 = time.time()\n",
    "print(\"Pred time: {}\".format(time1 - time0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91536\n",
      "f1 score: 0.91536\n",
      "Recall 0.91536\n",
      "Precision: 0.91536\n"
     ]
    }
   ],
   "source": [
    "def print_metrics(y_true, y_pred):\n",
    "    # accuracy\n",
    "    print(\"Accuracy: {}\".format(accuracy_score(y_true, y_pred)))\n",
    "    # f1\n",
    "    print(\"f1 score: {}\".format(f1_score(y_true, y_pred, average = \"micro\")))\n",
    "    #recall\n",
    "    print(\"Recall {}\".format(recall_score(y_true, y_pred, average = \"micro\")))\n",
    "    #sens\n",
    "    print(\"Precision: {}\".format(precision_score(y_true, y_pred, average = \"micro\")))\n",
    "    \n",
    "print_metrics(y_train, y_tr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "if os.path.isfile(\"rfMNIST.pkl\") == False:\n",
    "    rf_clf = RandomForestClassifier(n_estimators = 200, max_depth = 5, n_jobs = 6, random_state = 1, oob_score = True )\n",
    "    time0 = time.time()\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    time1 = time.time()\n",
    "    print(\"Train time: {}\".format(time1 - time0))\n",
    "else:\n",
    "    rf_clf = joblib.load(\"rfMNIST.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred time: 1.1285202503204346\n"
     ]
    }
   ],
   "source": [
    "time0 = time.time()\n",
    "y_tr_pred = rf_clf.predict(X_train)\n",
    "time1 = time.time()\n",
    "print(\"Pred time: {}\".format(time1 - time0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86314\n",
      "f1 score: 0.86314\n",
      "Recall 0.86314\n",
      "Precision: 0.86314\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_train, y_tr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85326"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " rf_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "if os.path.isfile(\"xgbMNIST.pkl\") == False:\n",
    "    xgb_clf = xgboost.XGBClassifier(objective='multi:softmax', n_jobs = 6)\n",
    "    time0 = time.time()\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "    time1 = time.time()\n",
    "    winsound.Beep(freq, duration)\n",
    "    print(\"Train time: {}\".format(time1 - time0))\n",
    "else:\n",
    "    xgb_clf = joblib.load(\"xgbMNIST.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred time: 1.07338547706604\n"
     ]
    }
   ],
   "source": [
    "time0 = time.time()\n",
    "y_tr_pred = xgb_clf.predict(X_train)\n",
    "time1 = time.time()\n",
    "print(\"Pred time: {}\".format(time1 - time0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94448\n",
      "f1 score: 0.94448\n",
      "Recall 0.94448\n",
      "Precision: 0.94448\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_train, y_tr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-Trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 0.6375987529754639\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "if os.path.isfile(\"etMNIST.pkl\") == False:\n",
    "    extra_clf = ExtraTreeClassifier()\n",
    "    time0 = time.time()\n",
    "    extra_clf.fit(X_train, y_train)\n",
    "    time1 = time.time()\n",
    "    winsound.Beep(freq, duration)\n",
    "    print(\"Train time: {}\".format(time1 - time0))\n",
    "else:\n",
    "     voting_clf = joblib.load(\"etMNIST.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the four models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['etMNIST.pkl']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(logreg_clf, \"logregMNIST.pkl\")\n",
    "joblib.dump(rf_clf, \"rfMNIST.pkl\")\n",
    "joblib.dump(xgb_clf, \"xgbMNIST.pkl\")\n",
    "joblib.dump(xgb_clf, \"etMNIST.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "\n",
    "I removed xgboost model because its peformance is a bit too good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 25.1208975315094\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"voting.pkl\") == False:\n",
    "    voting_clf = VotingClassifier(estimators = [(\"extra\",extra_clf), (\"rf\", rf_clf)], \n",
    "                              voting = \"hard\")\n",
    "    time0 = time.time()\n",
    "    y_tr_pred = voting_clf.fit(X_train, y_train)\n",
    "    time1 = time.time()\n",
    "    winsound.Beep(freq, duration)\n",
    "    print(\"Train time: {}\".format(time1 - time0))\n",
    "else:\n",
    "     voting_clf = joblib.load(\"voting.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred time: 1.9706382751464844\n"
     ]
    }
   ],
   "source": [
    "time0 = time.time()\n",
    "y_tr_pred = voting_clf.predict(X_train)\n",
    "time1 = time.time()\n",
    "print(\"Pred time: {}\".format(time1 - time0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreeClassifier 0.8227\n",
      "RandomForestClassifier 0.877\n",
      "VotingClassifier 0.8435\n"
     ]
    }
   ],
   "source": [
    "for clf in (extra_clf, rf_clf, voting_clf):\n",
    "    y_pred = clf.predict(X_val)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting is worse than a single model ¯\\\\_(ツ)_/¯ ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
